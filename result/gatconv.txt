Classifer(
  (gcn1): MultiHeadGATLayer(
    (heads): ModuleList(
      (0): GATLayer()
      (1): GATLayer()
    )
  )
  (gcn2): MultiHeadGATLayer(
    (heads): ModuleList(
      (0): GATLayer()
      (1): GATLayer()
    )
  )
)
c:\Users\mechrevo\Desktop\textgcn\mytrain.py:207: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  g.edata['w'] = F.softmax(e)
[2019/12/7 18:07:19] Epoch: 1, train_loss= 0.69410, train_acc= 0.49844, val_loss= 0.69322, val_acc= 0.55211, time= 0.99947
[2019/12/7 18:07:20] Epoch: 2, train_loss= 0.69082, train_acc= 0.56314, val_loss= 0.69265, val_acc= 0.68451, time= 0.70116
[2019/12/7 18:07:21] Epoch: 3, train_loss= 0.68273, train_acc= 0.69475, val_loss= 0.69169, val_acc= 0.72394, time= 0.67220
[2019/12/7 18:07:21] Epoch: 4, train_loss= 0.67164, train_acc= 0.77290, val_loss= 0.69104, val_acc= 0.74225, time= 0.71509
[2019/12/7 18:07:22] Epoch: 5, train_loss= 0.66257, train_acc= 0.81432, val_loss= 0.69046, val_acc= 0.75352, time= 0.66622
[2019/12/7 18:07:23] Epoch: 6, train_loss= 0.65336, train_acc= 0.84730, val_loss= 0.68992, val_acc= 0.76056, time= 0.67021
[2019/12/7 18:07:23] Epoch: 7, train_loss= 0.64412, train_acc= 0.87262, val_loss= 0.68944, val_acc= 0.76338, time= 0.71210
[2019/12/7 18:07:24] Epoch: 8, train_loss= 0.63532, train_acc= 0.88793, val_loss= 0.68902, val_acc= 0.76479, time= 0.68218
[2019/12/7 18:07:25] Epoch: 9, train_loss= 0.62712, train_acc= 0.89778, val_loss= 0.68865, val_acc= 0.76338, time= 0.69216
[2019/12/7 18:07:25] Epoch: 10, train_loss= 0.61956, train_acc= 0.90544, val_loss= 0.68835, val_acc= 0.77042, time= 0.68017
[2019/12/7 18:07:26] Epoch: 11, train_loss= 0.61276, train_acc= 0.91044, val_loss= 0.68816, val_acc= 0.77183, time= 0.69514
[2019/12/7 18:07:27] Epoch: 12, train_loss= 0.60670, train_acc= 0.91529, val_loss= 0.68808, val_acc= 0.76901, time= 0.67535
[2019/12/7 18:07:28] Epoch: 13, train_loss= 0.60126, train_acc= 0.92013, val_loss= 0.68809, val_acc= 0.78169, time= 0.70013
[2019/12/7 18:07:28] Epoch: 14, train_loss= 0.59641, train_acc= 0.92654, val_loss= 0.68817, val_acc= 0.78169, time= 0.66422
[2019/12/7 18:07:29] Epoch: 15, train_loss= 0.59209, train_acc= 0.93185, val_loss= 0.68829, val_acc= 0.77887, time= 0.69215
[2019/12/7 18:07:30] Epoch: 16, train_loss= 0.58818, train_acc= 0.93764, val_loss= 0.68846, val_acc= 0.78028, time= 0.67519
[2019/12/7 18:07:30] Epoch: 17, train_loss= 0.58463, train_acc= 0.94139, val_loss= 0.68868, val_acc= 0.78028, time= 0.70511
[2019/12/7 18:07:30] Early stopping...
[2019/12/7 18:07:30] Optimization Finished!
[2019/12/7 18:07:30] Test set results:
[2019/12/7 18:07:30]     loss= 0.67299, accuracy= 0.76646, time= 0.23836
[2019/12/7 18:07:31] Test Precision, Recall and F1-Score...
[2019/12/7 18:07:31]               precision    recall  f1-score   support
[2019/12/7 18:07:31]
[2019/12/7 18:07:31]            0     0.7748    0.7513    0.7629      1777
[2019/12/7 18:07:31]            1     0.7586    0.7817    0.7700      1777
[2019/12/7 18:07:31]
[2019/12/7 18:07:31]     accuracy                         0.7665      3554
[2019/12/7 18:07:31]    macro avg     0.7667    0.7665    0.7664      3554
[2019/12/7 18:07:31] weighted avg     0.7667    0.7665    0.7664      3554
[2019/12/7 18:07:31]
[2019/12/7 18:07:31] Macro average Test Precision, Recall and F1-Score...
[2019/12/7 18:07:31] (0.766706616208314, 0.7664603263927968, 0.7664063984795693, None)
[2019/12/7 18:07:31] Micro average Test Precision, Recall and F1-Score...
[2019/12/7 18:07:31] (0.7664603263927968, 0.7664603263927968, 0.7664603263927969, None)
[2019/12/7 18:07:31] Embeddings:
Word_embeddings:18764
Train_doc_embeddings:7108
Test_doc_embeddings:3554
Word_embeddings::31]
[[0.         0.         0.         ... 0.01069007 0.         0.        ]
 [0.1075963  0.         0.22394033 ... 0.15131849 0.         0.        ]
 [0.         0.1523975  0.         ... 0.13892259 0.03494497 0.01531071]
 ...
 [0.         0.         0.         ... 0.         0.         0.        ]
 [0.16102764 0.03472325 0.         ... 0.         0.         0.12128486]
 [0.         0.         0.         ... 0.         0.         0.        ]]